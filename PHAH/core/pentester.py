#!/usr/bin/env python3
"""
Módulo Principal de Auto Pentester
Orquesta pruebas de penetración automatizadas usando IA
"""

import asyncio
from datetime import datetime
from typing import Optional, List, Dict, Any
from pathlib import Path

from .ollama_client import OllamaClient
from .report_generator import ReportGenerator
from tools.command_executor import CommandExecutor


class AutoPentester:
  """Clase principal para pruebas de penetración automatizadas"""

  def __init__(
    self,
    service: str,
    target: str,
    port: Optional[int] = None,
    model: str = "llama3.2",
    verbose: bool = True,
    generate_report: bool = False
  ):
    """
    Inicializa Auto Pentester

    Args:
      service: Tipo de servicio (web, ssh, samba, etc.)
      target: Host o URL objetivo
      port: Puerto objetivo (opcional, usa valores por defecto si no se especifica)
      model: Modelo de Ollama a usar
      verbose: Mostrar salida detallada
      generate_report: Generar reporte después de la prueba
    """
    self.service = service.lower()
    self.target = target
    self.port = port
    self.model = model
    self.verbose = verbose
    self.generate_report_flag = generate_report

    # Componentes
    self.ollama_client = OllamaClient(model=model)
    self.command_executor = CommandExecutor(verbose=verbose)
    self.report_generator = ReportGenerator()

    # Datos de prueba
    self.findings: List[Dict[str, Any]] = []
    self.start_time: Optional[datetime] = None
    self.end_time: Optional[datetime] = None

  def _print(self, message: str, prefix: str = ""):
    """Mostrar mensaje si verbose está activo"""
    if self.verbose:
      timestamp = datetime.now().strftime("%H:%M:%S")
      if prefix:
        print(f"[{timestamp}] [{prefix}] {message}")
      else:
        print(f"[{timestamp}] {message}")

  async def load_service_prompt(self) -> str:
    """
    Cargar prompt del sistema para el servicio específico

    Returns:
      Contenido del prompt del sistema
    """
    prompt_file = Path(__file__).parent.parent / "prompts" / f"{self.service}_pentester.md"

    if prompt_file.exists():
      with open(prompt_file, 'r', encoding='utf-8') as f:
        return f.read()
    else:
      # Prompt genérico por defecto
      return f"""Eres un experto probador de penetración especializado en pruebas de seguridad de {self.service.upper()}.

Tu función es:
1. Realizar un reconocimiento exhaustivo del objetivo
2. Identificar vulnerabilidades potenciales
3. Probar problemas de seguridad comunes
4. Proporcionar hallazgos claros y accionables
5. Sugerir pasos de corrección

Información del Objetivo:
- Servicio: {self.service.upper()}
- Objetivo: {self.target}
- Puerto: {self.port if self.port else 'Por defecto'}

IMPORTANTE:
- Solo ejecuta comandos que sean seguros y autorizados
- Céntrate en el reconocimiento e identificación de vulnerabilidades
- Documenta todos los hallazgos claramente
- Proporciona calificaciones de gravedad (CRÍTICO, ALTO, MEDIO, BAJO, INFO)

Presenta tu análisis en un formato estructurado con:
- Título del hallazgo
- Nivel de gravedad
- Descripción
- Evidencia
- Recomendación

Comencemos la evaluación de seguridad."""

  async def run(self) -> Dict[str, Any]:
    """
    Ejecutar la prueba de penetración automatizada

    Returns:
      Diccionario con resultados de la prueba
    """
    self.start_time = datetime.now()
    self._print("=" * 80, "PHAH")
    self._print(f"Iniciando prueba de penetración automatizada", "PHAH")
    self._print(f"Servicio: {self.service.upper()}", "PHAH")
    self._print(f"Objetivo: {self.target}", "PHAH")
    self._print(f"Puerto: {self.port if self.port else 'Por defecto'}", "PHAH")
    self._print("=" * 80, "PHAH")

    try:
      # Cargar prompt específico del servicio
      system_prompt = await self.load_service_prompt()

      # Inicializar conversación con el LLM
      self._print("Inicializando pentester de IA...", "IA")
      initial_message = f"Comienza la evaluación de seguridad del servicio {self.service.upper()} en {self.target}"
      if self.port:
        initial_message += f" puerto {self.port}"

      response = await self.ollama_client.chat(
        message=initial_message,
        system_prompt=system_prompt,
        temperature=0.7
      )

      self._print("Pentester de IA inicializado", "IA")
      if self.verbose:
        print(f"\n{response}\n")

      # Bucle iterativo de pentesting
      max_iterations = 10
      iteration = 0

      while iteration < max_iterations:
        iteration += 1
        self._print(f"Iteración {iteration}/{max_iterations}", "BUCLE")

        # Pedir al LLM el siguiente comando a ejecutar
        next_action_prompt = """Basándote en los hallazgos actuales, ¿cuál es el siguiente comando que quieres ejecutar?

Responde con SOLO UNA de las siguientes opciones:
1. Un único comando a ejecutar (ej: "nmap -sV objetivo.com")
2. "ANALISIS" si quieres analizar hallazgos sin ejecutar más comandos
3. "HECHO" si la evaluación está completa

Tu respuesta:"""

        action_response = await self.ollama_client.chat(
          message=next_action_prompt,
          temperature=0.5
        )

        action = action_response.strip()

        # Comprobar si ha terminado
        if "HECHO" in action.upper():
          self._print("Evaluación completa", "IA")
          break

        # Comprobar si quiere análisis
        if "ANALISIS" in action.upper():
          self._print("Realizando análisis final...", "IA")
          analysis_prompt = "Por favor, proporciona un análisis exhaustivo de todos los hallazgos, incluyendo niveles de gravedad y recomendaciones."
          final_analysis = await self.ollama_client.chat(
            message=analysis_prompt,
            temperature=0.7
          )
          if self.verbose:
            print(f"\n{final_analysis}\n")
          break

        # Extraer comando (intentar analizarlo)
        command = self._extract_command(action)

        if command:
          # Ejecutar comando
          self._print(f"Ejecutando: {command}", "CMD")
          output = await self.command_executor.execute_and_get_output(command)

          # Mostrar salida si verbose está activo
          if self.verbose and output:
            print(f"\n--- Salida ---\n{output}\n--- Fin Salida ---\n")

          # Enviar resultados de vuelta al LLM
          analysis_prompt = f"Analiza la salida del comando '{command}' e identifica cualquier hallazgo de seguridad."
          analysis = await self.ollama_client.chat_with_tools(
            message=analysis_prompt,
            tools_output=output,
            temperature=0.7
          )

          if self.verbose:
            print(f"\n{analysis}\n")
        else:
          self._print("No se pudo extraer un comando válido, continuando...", "AVISO")

      # Resumen final
      self.end_time = datetime.now()
      self._print("=" * 80, "PHAH")
      self._print("Prueba de penetración completada", "PHAH")
      self._print(f"Duración: {self.end_time - self.start_time}", "PHAH")
      self._print("=" * 80, "PHAH")

      # Generar reporte si se solicita
      if self.generate_report_flag:
        await self.generate_report()

      return {
        "success": True,
        "service": self.service,
        "target": self.target,
        "port": self.port,
        "findings": self.findings,
        "start_time": self.start_time,
        "end_time": self.end_time
      }

    except KeyboardInterrupt:
      self._print("Prueba interrumpida por el usuario", "AVISO")
      return {"success": False, "error": "Interrumpido"}
    except Exception as e:
      self._print(f"Error durante la prueba: {str(e)}", "ERROR")
      return {"success": False, "error": str(e)}
    finally:
      await self.ollama_client.close()

  def _extract_command(self, text: str) -> Optional[str]:
    """
    Extraer comando de la respuesta del LLM

    Args:
      text: Texto de respuesta del LLM

    Returns:
      Comando extraído o None
    """
    # Eliminar bloques de código markdown comunes
    text = text.strip()

    # Eliminar marcadores ``` o ```bash
    if "```" in text:
      lines = text.split('\n')
      command_lines = []
      in_code_block = False

      for line in lines:
        if line.strip().startswith("```"):
          in_code_block = not in_code_block
          continue
        if in_code_block and line.strip():
          command_lines.append(line.strip())

      if command_lines:
        return command_lines[0]

    # Intentar encontrar patrones que parezcan comandos
    lines = text.split('\n')
    for line in lines:
      line = line.strip()
      # Saltar líneas vacías y líneas que parezcan explicaciones
      if not line or line.endswith(':') or line.endswith('?'):
        continue
      # Comandos comunes de inicio
      if any(line.startswith(cmd) for cmd in ['nmap', 'curl', 'nikto', 'dirb', 'gobuster',
                                                'ssh', 'smbclient', 'enum4linux', 'hydra',
                                                'whatweb', 'wpscan', 'sqlmap', 'nc', 'netcat']):
        return line

    return None

  async def generate_report(self):
    """Generar reporte de prueba de penetración"""
    self._print("Generando reporte...", "REPORTE")

    # Obtener historial de conversación para análisis
    llm_history = self.ollama_client.get_history()

    # Extraer análisis final del LLM
    llm_analysis = ""
    for msg in reversed(llm_history):
      if msg.get("role") == "assistant":
        content = msg.get("content", "")
        if len(content) > 100:  # Obtener respuesta sustancial
          llm_analysis = content
          break

    # Obtener historial de comandos
    commands = [cmd["command"] for cmd in self.command_executor.get_history()]

    # Generar reportes en múltiples formatos
    md_report = self.report_generator.generate_markdown_report(
      service=self.service,
      target=self.target,
      port=self.port,
      findings=self.findings,
      commands_executed=commands,
      llm_analysis=llm_analysis,
      start_time=self.start_time,
      end_time=self.end_time
    )

    html_report = self.report_generator.generate_html_report(
      service=self.service,
      target=self.target,
      port=self.port,
      findings=self.findings,
      commands_executed=commands,
      llm_analysis=llm_analysis,
      start_time=self.start_time,
      end_time=self.end_time
    )

    json_report = self.report_generator.generate_json_report(
      service=self.service,
      target=self.target,
      port=self.port,
      findings=self.findings,
      commands_executed=commands,
      llm_analysis=llm_analysis,
      start_time=self.start_time,
      end_time=self.end_time
    )

    self._print(f"Reportes generados:", "REPORTE")
    self._print(f"  - Markdown: {md_report}", "REPORTE")
    self._print(f"  - HTML: {html_report}", "REPORTE")
    self._print(f"  - JSON: {json_report}", "REPORTE")
