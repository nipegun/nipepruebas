#!/usr/bin/env python3

import json
import requests
import asyncio
import websockets
import sys
import os
import re
import time
import shutil
import subprocess

# --------------------------------------------------------
# CONFIGURACIÓN
# --------------------------------------------------------

# Obtener ruta absoluta del directorio del orchestrator
vRutaBaseOrchestrator = os.path.dirname(os.path.abspath(__file__))
vRutaTools = os.path.join(vRutaBaseOrchestrator, "tools.txt")
vRutaSystemPrompt = os.path.join(vRutaBaseOrchestrator, "system-prompt.txt")
vRutaRequirements = os.path.join(vRutaBaseOrchestrator, "requirements-python.txt")

# URL de la API de IA - lee de variable de entorno o usa valor por defecto
vURLLlama = os.environ.get("PHIA_AI_API_URL", "http://127.0.0.1:9000/completion")
vProveedorIA = os.environ.get("PHIA_AI_PROVIDER", "llama.cpp")
vAPIKey = os.environ.get("PHIA_AI_API_KEY", None)
vWSMCP = "ws://127.0.0.1:9002"

vRutaBaseWorkspace = "$HOME/PenTests"

# Globals de workspace
vWorkspace = None
vRutaLogs = None
vRutaEvidence = None
vRutaExploits = None
vRutaReport = None

vContadorAcciones = 0


# --------------------------------------------------------
# UTILIDADES DE FICHEROS Y NOMBRES
# --------------------------------------------------------

def fNormalizarNombre(vTexto):
  vTexto = vTexto.strip().lower()
  vTexto = re.sub(r"[^a-z0-9]+", "_", vTexto)
  vTexto = re.sub(r"_+", "_", vTexto)
  vTexto = vTexto.strip("_")
  if not vTexto:
    vTexto = "objetivo"
  return vTexto

def fAsegurarDirectorio(vRuta):
  if not os.path.exists(vRuta):
    os.makedirs(vRuta, exist_ok=True)

def fGuardarJSONEnArchivo(vRutaArchivo, vObjeto):
  with open(vRutaArchivo, "w") as f:
    json.dump(vObjeto, f, indent=2)

def fGuardarTextoEnArchivo(vRutaArchivo, vTexto):
  with open(vRutaArchivo, "w") as f:
    f.write(vTexto)


# --------------------------------------------------------
# VERIFICACIÓN DE DEPENDENCIAS
# --------------------------------------------------------

def fCargarRequirements():
  if not os.path.exists(vRutaRequirements):
    return []

  aPaquetes = []
  with open(vRutaRequirements, "r") as f:
    for vLinea in f:
      vLinea = vLinea.strip()
      if vLinea and not vLinea.startswith("#"):
        aPaquetes.append(vLinea)

  return aPaquetes

def fVerificarPaquete(vPaquete):
  """
  Verifica si un paquete de Python está instalado
  """
  try:
    # Intentar importar el paquete
    __import__(vPaquete)
    return True
  except ImportError:
    return False

def fVerificarDependencias():
  aPaquetes = fCargarRequirements()

  if not aPaquetes:
    return

  print("\n" + "="*60)
  print("VERIFICANDO DEPENDENCIAS DE PENTESTING")
  print("="*60)

  aInstalados = []
  aFaltantes = []

  for vPaq in aPaquetes:
    if fVerificarPaquete(vPaq):
      aInstalados.append(vPaq)
      print(f"✓ {vPaq:<20} [INSTALADO]")
    else:
      aFaltantes.append(vPaq)
      print(f"✗ {vPaq:<20} [NO ENCONTRADO]")

  print("="*60)
  print(f"Instalados: {len(aInstalados)}/{len(aPaquetes)}")

  if aFaltantes:
    print(f"\nPaquetes faltantes ({len(aFaltantes)}):")
    for vPaq in aFaltantes:
      print(f"  - {vPaq}")

    print("\nPara instalar los paquetes faltantes, ejecuta:")
    print(f"  pip3 install {' '.join(aFaltantes)}")

    vRespuesta = input("\n¿Deseas instalar los paquetes faltantes ahora? (s/N): ").strip().lower()

    if vRespuesta in ['s', 'si', 'y', 'yes']:
      print("\nInstalando paquetes de Python...")
      try:
        vCmd = [sys.executable, "-m", "pip", "install"] + aFaltantes
        vResult = subprocess.run(vCmd)

        if vResult.returncode == 0:
          print("\n✓ Paquetes instalados correctamente.")
        else:
          print("\n✗ Hubo un error durante la instalación.")
          print("  Puedes instalarlos manualmente más tarde.")
      except Exception as e:
        print(f"\n✗ Error al instalar: {e}")
        print("  Instálalos manualmente con:")
        print(f"  pip3 install {' '.join(aFaltantes)}")
    else:
      print("\nPuedes instalarlos más tarde con el comando mostrado arriba.")
  else:
    print("\n✓ Todas las dependencias están instaladas.")

  print("="*60 + "\n")


# --------------------------------------------------------
# WORKSPACE
# --------------------------------------------------------

def fCrearWorkspace(vTarget):
  global vWorkspace, vRutaLogs, vRutaEvidence, vRutaExploits, vRutaReport

  vNombre = fNormalizarNombre(vTarget)
  vWorkspace = os.path.join(vRutaBaseWorkspace, vNombre)

  vRutaLogs = os.path.join(vWorkspace, "logs")
  vRutaEvidence = os.path.join(vWorkspace, "evidence")
  vRutaExploits = os.path.join(vWorkspace, "exploits")
  vRutaReport = os.path.join(vWorkspace, "report")

  fAsegurarDirectorio(vWorkspace)
  fAsegurarDirectorio(vRutaLogs)
  fAsegurarDirectorio(vRutaEvidence)
  fAsegurarDirectorio(vRutaExploits)
  fAsegurarDirectorio(vRutaReport)

  print(f"Workspace creado: {vWorkspace}")
  print(f"  Logs:      {vRutaLogs}")
  print(f"  Evidence:  {vRutaEvidence}")
  print(f"  Exploits:  {vRutaExploits}")
  print(f"  Report:    {vRutaReport}")


# --------------------------------------------------------
# CARGA DE TOOLS Y SYSTEM PROMPT
# --------------------------------------------------------

def fCargarTools():
  if not os.path.exists(vRutaTools):
    print(f"No encuentro {vRutaTools}")
    sys.exit(1)

  aTools = []
  vActual = {}

  with open(vRutaTools, "r") as f:
    for vLinea in f:
      vLinea = vLinea.strip()
      if not vLinea:
        if vActual:
          aTools.append(vActual)
          vActual = {}
        continue

      if vLinea.startswith("NAME="):
        vActual["name"] = vLinea.split("=",1)[1]
      elif vLinea.startswith("DESC="):
        vActual["desc"] = vLinea.split("=",1)[1]
      elif vLinea.startswith("ARGS="):
        vActual["args"] = vLinea.split("=",1)[1]

    if vActual:
      aTools.append(vActual)

  return aTools

def fCargarSystemPrompt():
  if not os.path.exists(vRutaSystemPrompt):
    print(f"No encuentro {vRutaSystemPrompt}")
    sys.exit(1)

  with open(vRutaSystemPrompt, "r") as f:
    return f.read()

def fConstruirPromptTotal(vSystemPromptBase, aTools):
  aLineas = []
  aLineas.append(vSystemPromptBase.strip())
  aLineas.append("\n\n=== HERRAMIENTAS MCP DISPONIBLES ===\n")

  for vTool in aTools:
    vNombre = vTool["name"]
    vDesc = vTool.get("desc", "Sin descripción")
    aArgs = vTool.get("args", "").split(",")

    aLineas.append(f"- {vNombre}")
    aLineas.append(f"  Descripción: {vDesc}")
    aLineas.append("  Uso (formato JSON):")
    aLineas.append("  {")
    aLineas.append(f'    "tool": "{vNombre}",')
    aLineas.append('    "arguments": {')
    for vArg in aArgs:
      vArg = vArg.strip()
      if vArg:
        aLineas.append(f'      "{vArg}": "VALOR",')
    aLineas.append("    }")
    aLineas.append("  }\n")

  aLineas.append("Cuando quieras usar una herramienta, devuelve SOLO el JSON exacto del tool-call.")
  aLineas.append("No expliques el comando, no des texto adicional, solo el JSON.")

  return "\n".join(aLineas)


# --------------------------------------------------------
# COMUNICACIÓN CON LLAMA.CPP
# --------------------------------------------------------

def fEnviarALlama(vPromptCompleto, vEntradaUsuario):
  vPromptFinal = vPromptCompleto + "\n\nUsuario:\n" + vEntradaUsuario + "\nAsistente:"

  vPayload = {
    "prompt": vPromptFinal,
    "temperature": 0.0,  # Más determinista
    "top_p": 0.9,
    "top_k": 40,
    "n_predict": 300,  # Reducido para respuestas más cortas
    "stop": ["\nUsuario:", "Usuario:", "\n\n"]  # Detener en estas cadenas
  }

  print(f"\n[DEBUG] Enviando petición a {vURLLlama}")
  print(f"[DEBUG] Longitud del prompt: {len(vPromptFinal)} caracteres")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida")

    # Ajusta esta parte según el formato real de la respuesta de tu llama.cpp.
    if "content" in vJSON:
      return vJSON["content"]
    else:
      # Fallback simple
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con llama.cpp")
    return "ERROR: El modelo tardó demasiado en responder."
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con llama.cpp: {e}")
    return f"ERROR: {str(e)}"


def fEnviarADeepSeek(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a la API de DeepSeek usando formato compatible con OpenAI
  """
  if not vAPIKey:
    return "ERROR: Se requiere API key para usar DeepSeek. Configura PHIA_AI_API_KEY."

  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "deepseek-chat",
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300,
    "stream": False
  }

  vHeaders = {
    "Authorization": f"Bearer {vAPIKey}",
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a DeepSeek API")
  print(f"[DEBUG] Modelo: deepseek-chat")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de DeepSeek")

    # La respuesta de DeepSeek sigue el formato de OpenAI
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con DeepSeek API")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de DeepSeek: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con DeepSeek: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAOpenAI(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a la API de OpenAI usando formato compatible
  """
  if not vAPIKey:
    return "ERROR: Se requiere API key para usar OpenAI. Configura PHIA_AI_API_KEY."

  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "gpt-4o-mini",  # Modelo más económico y rápido
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300
  }

  vHeaders = {
    "Authorization": f"Bearer {vAPIKey}",
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a OpenAI API")
  print(f"[DEBUG] Modelo: gpt-4o-mini")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de OpenAI")

    # La respuesta de OpenAI sigue el formato estándar
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con OpenAI API")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de OpenAI: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con OpenAI: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAOllama(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a Ollama usando formato compatible con OpenAI
  """
  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "llama3.2",  # Modelo por defecto, puede cambiarse
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300,
    "stream": False
  }

  vHeaders = {
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a Ollama")
  print(f"[DEBUG] Modelo: llama3.2")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de Ollama")

    # Ollama usa formato compatible con OpenAI
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    # Formato alternativo de Ollama (sin /v1/)
    elif "message" in vJSON and "content" in vJSON["message"]:
      return vJSON["message"]["content"]
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con Ollama")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.ConnectionError:
    print(f"[ERROR] No se puede conectar con Ollama")
    print(f"[ERROR] Asegúrate de que Ollama esté corriendo: ollama serve")
    return "ERROR: No se puede conectar con Ollama. ¿Está corriendo?"
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de Ollama: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con Ollama: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAVLLM(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a vLLM usando formato compatible con OpenAI
  """
  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "model",  # vLLM ignora este campo pero es requerido
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300,
    "stream": False
  }

  vHeaders = {
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a vLLM")
  print(f"[DEBUG] URL: {vURLLlama}")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de vLLM")

    # vLLM usa formato compatible con OpenAI
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con vLLM")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.ConnectionError:
    print(f"[ERROR] No se puede conectar con vLLM")
    print(f"[ERROR] Asegúrate de que vLLM esté corriendo en: {vURLLlama}")
    return "ERROR: No se puede conectar con vLLM. ¿Está corriendo?"
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de vLLM: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con vLLM: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAClaude(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a la API de Claude (Anthropic) usando su formato nativo
  """
  if not vAPIKey:
    return "ERROR: Se requiere API key para usar Claude. Configura PHIA_AI_API_KEY."

  # Claude usa un formato diferente: system es un parámetro separado
  aMensajes = [
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "claude-3-5-sonnet-20241022",  # Modelo más reciente
    "system": vPromptCompleto,  # System prompt es parámetro separado en Claude
    "messages": aMensajes,
    "max_tokens": 300,  # Requerido por Claude
    "temperature": 0.0  # Determinista
  }

  # Claude usa headers diferentes
  vHeaders = {
    "x-api-key": vAPIKey,  # Claude usa x-api-key en lugar de Authorization Bearer
    "anthropic-version": "2023-06-01",  # Versión de la API requerida
    "content-type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a Claude API")
  print(f"[DEBUG] Modelo: claude-3-5-sonnet-20241022")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de Claude")

    # Claude devuelve el contenido en content[0].text
    if "content" in vJSON and len(vJSON["content"]) > 0:
      vContenido = vJSON["content"][0]["text"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con Claude API")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de Claude: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con Claude: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAIA(vPromptCompleto, vEntradaUsuario):
  """
  Función dispatcher que envía el prompt al proveedor de IA configurado
  """
  if vProveedorIA == "ollama":
    return fEnviarAOllama(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "vllm":
    return fEnviarAVLLM(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "deepseek":
    return fEnviarADeepSeek(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "openai":
    return fEnviarAOpenAI(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "claude":
    return fEnviarAClaude(vPromptCompleto, vEntradaUsuario)
  else:
    # Por defecto usa llama.cpp
    return fEnviarALlama(vPromptCompleto, vEntradaUsuario)


# --------------------------------------------------------
# COMUNICACIÓN CON MCP
# --------------------------------------------------------

async def fVerificarMCPServer():
  """Verifica si el MCP server está corriendo"""
  try:
    async with websockets.connect(vWSMCP, open_timeout=2) as vWS:
      # Si la conexión es exitosa, el server está corriendo
      return True
  except Exception:
    return False

async def fLlamarMCP(vTool, vArgs):
  try:
    async with websockets.connect(vWSMCP) as vWS:
      vMsg = {
        "jsonrpc": "2.0",
        "id": 1,
        "method": "tools/call",
        "params": {
          "tool": vTool,
          "arguments": vArgs
        }
      }
      await vWS.send(json.dumps(vMsg))
      vRespuesta = await vWS.recv()
      return json.loads(vRespuesta)
  except ConnectionRefusedError:
    print("\n" + "="*70)
    print("ERROR: MCP SERVER NO ESTÁ CORRIENDO")
    print("="*70)
    print("El orquestador no puede conectarse al MCP server en:")
    print(f"  {vWSMCP}")
    print("\nPara iniciar el MCP server, abre otra terminal y ejecuta:")
    print(f"  cd {os.path.join(os.path.dirname(os.path.abspath(__file__)))}")
    print("  ./mcp-server-start.sh")
    print("\nLuego vuelve a ejecutar el framework.")
    print("="*70 + "\n")
    sys.exit(1)
  except Exception as e:
    print(f"\n[ERROR] Error al comunicarse con MCP server: {e}\n")
    return {"error": str(e)}


# --------------------------------------------------------
# DETECTAR TOOL-CALL DEL MODELO
# --------------------------------------------------------

def fDetectarToolCall(vTexto):
  """
  Detecta y extrae un tool-call JSON del texto del modelo.
  El JSON puede venir mezclado con texto adicional.
  """
  # Primero intenta parsear todo el texto como JSON
  try:
    vJSON = json.loads(vTexto.strip())
    if "tool" in vJSON and "arguments" in vJSON:
      return vJSON
  except Exception:
    pass

  # Buscar todas las posiciones donde aparece '{'
  vPosiciones = [i for i, c in enumerate(vTexto) if c == '{']

  # Para cada '{', intentar extraer un JSON válido
  for vInicio in vPosiciones:
    # Intentar encontrar el '}' correspondiente
    vContador = 0
    for vFin in range(vInicio, len(vTexto)):
      if vTexto[vFin] == '{':
        vContador += 1
      elif vTexto[vFin] == '}':
        vContador -= 1
        if vContador == 0:
          # Encontramos un par completo de llaves
          vJSONStr = vTexto[vInicio:vFin+1]
          try:
            vJSON = json.loads(vJSONStr)
            if isinstance(vJSON, dict) and "tool" in vJSON and "arguments" in vJSON:
              print(f"[DEBUG] Tool-call detectado: {vJSON['tool']}")
              print(f"[DEBUG] JSON extraído: {vJSONStr}")
              return vJSON
          except Exception:
            # Este fragmento no es JSON válido, continuar
            pass
          break

  print(f"[DEBUG] No se detectó tool-call válido en la respuesta")
  print(f"[DEBUG] Texto recibido: {vTexto[:200]}...")
  return None


# --------------------------------------------------------
# PROCESAR RESULTADO DE MCP (LOGS, EVIDENCIAS, INFORME)
# --------------------------------------------------------

def fGenerarNombreAccion(vTool):
  global vContadorAcciones
  vContadorAcciones += 1
  vTimestamp = int(time.time())
  vNombre = f"{vContadorAcciones:04d}_{fNormalizarNombre(vTool)}_{vTimestamp}"
  return vNombre

def fProcesarResultadoMCP(vTool, vArgs, vResultado):
  if vWorkspace is None:
    return

  vNombreAccion = fGenerarNombreAccion(vTool)

  # 1) Log general de tool-call (args + resultado bruto)
  vRutaLogJSON = os.path.join(vRutaLogs, f"{vNombreAccion}_toolcall.json")
  vObjetoLog = {
    "tool": vTool,
    "arguments": vArgs,
    "result": vResultado
  }
  fGuardarJSONEnArchivo(vRutaLogJSON, vObjetoLog)

  # 2) Si hay request/response HTTP, los guardamos como evidencias
  try:
    vResult = vResultado.get("result", {})
    vOutput = vResult.get("output", {})

    vRequest = vOutput.get("request")
    vResponse = vOutput.get("response")

    if vRequest:
      vRutaReq = os.path.join(vRutaEvidence, f"{vNombreAccion}_request.txt")
      fGuardarTextoEnArchivo(vRutaReq, vRequest)

    if vResponse:
      vRutaRes = os.path.join(vRutaEvidence, f"{vNombreAccion}_response.txt")
      if isinstance(vResponse, str):
        fGuardarTextoEnArchivo(vRutaRes, vResponse)
      else:
        fGuardarTextoEnArchivo(vRutaRes, json.dumps(vResponse, indent=2))

  except Exception:
    pass

  # 3) Opcional: si el MCP devuelve algo como "evidence" o "note", también lo guardamos
  try:
    vResult = vResultado.get("result", {})
    vEvidence = vResult.get("evidence")
    if vEvidence:
      vRutaEv = os.path.join(vRutaEvidence, f"{vNombreAccion}_extra.txt")
      if isinstance(vEvidence, str):
        fGuardarTextoEnArchivo(vRutaEv, vEvidence)
      else:
        fGuardarTextoEnArchivo(vRutaEv, json.dumps(vEvidence, indent=2))
  except Exception:
    pass


# --------------------------------------------------------
# MODO AUTOMÁTICO
# --------------------------------------------------------

async def fModoAutomatico(vTarget, vSystemPromptConObjetivo):
  """
  Ejecuta automáticamente una secuencia completa de pentesting
  """
  print("\n" + "="*70)
  print("MODO AUTOMÁTICO ACTIVADO")
  print("="*70)
  print("El framework ejecutará automáticamente:")
  print("  1. Reconocimiento inicial (Scan de Puertos/Servicios)")
  print("  2. Análisis de vulnerabilidades")
  print("  3. Intentos de explotación básicos")
  print("  4. Generación de informe final")
  print("="*70 + "\n")

  # FASE 0: PREPARACIÓN
  await fLlamarMCP("report.write", {
    "workspace": vWorkspace,
    "section": "OBJETIVO",
    "content": f"Objetivo del pentest: {vTarget}"
  })

  # Bucle principal de razonamiento
  vContexto = vSystemPromptConObjetivo
  aHistorial = []

  for i in range(15): # Límite de pasos para evitar bucles infinitos
    print(f"\n[PASO {i+1}] Analizando situación...")

    # Construir historial para el prompt
    vHistorialTexto = ""
    for vMsg in aHistorial:
      vHistorialTexto += f"{vMsg['role']}: {vMsg['content']}\n"

    # Consultar a la IA qué hacer
    vRespuestaIA = fEnviarAIA(vContexto, vHistorialTexto + "\n\nAnaliza los hallazgos previos y decide el siguiente paso. Si has terminado o encontrado vulnerabilidades críticas, genera el reporte final.")

    print(f"[IA] Pensamiento: {vRespuestaIA[:100]}...")

    # Detectar si quiere ejecutar una herramienta
    vToolCall = fDetectarToolCall(vRespuestaIA)

    if vToolCall:
      vToolName = vToolCall["tool"]
      vArgs = vToolCall["arguments"]

      # Inyectar workspace si hace falta
      if "workspace" not in vArgs:
        vArgs["workspace"] = vWorkspace

      print(f"[EJECUCIÓN] {vToolName} {vArgs}")
      vResultado = await fLlamarMCP(vToolName, vArgs)

      fProcesarResultadoMCP(vToolName, vArgs, vResultado)

      # Añadir al historial
      aHistorial.append({"role": "Assistant", "content": vRespuestaIA})
      aHistorial.append({"role": "System", "content": f"Result of {vToolName}: {json.dumps(vResultado)}"})

      # Si generó reporte final, terminamos
      if vToolName == "report.final_md" or vToolName == "report.final_html":
        print("\n[FIN] Informe final generado.")
        break

    else:
      # Si no llama a herramienta, solo conversamos (o la IA está confundida)
      print(f"[IA] Mensaje: {vRespuestaIA}")
      aHistorial.append({"role": "Assistant", "content": vRespuestaIA})
      
      # Si la IA dice explícitamente que terminó
      if "TERMINADO" in vRespuestaIA or "FINISHED" in vRespuestaIA:
        break

  print("\n" + "="*70)
  print("Pentesting Automático Finalizado")
  print(f"Informe disponible en: {vRutaReport}")
  print("="*70 + "\n")


# --------------------------------------------------------
# MAIN
# --------------------------------------------------------

async def main():
  print("=======================================")
  print("  PHIA Pentester - LAN FRAMEWORK")
  print("=======================================")

  fVerificarDependencias()

  if not await fVerificarMCPServer():
    print("Iniciando servidor MCP...")
    # Intentamos iniciarlo nosotros si no está corriendo (opcional)
    # Pero por diseño, preferimos pedirle al usuario que lo inicie
    print("ERROR: El servidor MCP no está respondiendo.")
    print(f"Asegúrate de ejecutar ./mcp-server-start.sh en otra terminal.")
    sys.exit(1)

  print("Servidor MCP detectado: ONLINE")

  # Obtener target: primero desde variable de entorno, luego desde argumentos, finalmente preguntar
  vTarget = os.environ.get("PHIA_TARGET")

  if not vTarget and len(sys.argv) >= 2:
    vTarget = sys.argv[1]

  if not vTarget:
    vTarget = input("\nObjetivo (IP o rango de red, ej: 192.168.1.0/24 o 192.168.1.1): ").strip()

  if not vTarget:
    print("Objetivo vacío. Saliendo.")
    sys.exit(1)

  fCrearWorkspace(vTarget)

  # Cargar recursos
  try:
    aTools = fCargarTools()
    vSystemPrompt = fCargarSystemPrompt()
    vPromptTotal = fConstruirPromptTotal(vSystemPrompt, aTools)
  except Exception as e:
    print(f"Error cargando recursos: {e}")
    sys.exit(1)

  # Prompt inicial específico con el objetivo
  vPromptConObjetivo = (
    f"{vPromptTotal}\n\n"
    f"EL OBJETIVO ACTUAL ES: {vTarget}\n"
    f"Inicia el reconocimiento y ataca según los hallazgos.\n"
  )

  await fModoAutomatico(vTarget, vPromptConObjetivo)

if __name__ == "__main__":
  asyncio.run(main())
