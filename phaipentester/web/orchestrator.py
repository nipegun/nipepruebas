#!/usr/bin/env python3

import json
import requests
import asyncio
import websockets
import sys
import os
import re
import time
import shutil
import subprocess

# --------------------------------------------------------
# CONFIGURACIÓN
# --------------------------------------------------------

# Obtener ruta absoluta del directorio del orchestrator
vRutaBaseOrchestrator = os.path.dirname(os.path.abspath(__file__))
vRutaTools = os.path.join(vRutaBaseOrchestrator, "tools.txt")
vRutaSystemPrompt = os.path.join(vRutaBaseOrchestrator, "system-prompt.txt")
vRutaRequirements = os.path.join(vRutaBaseOrchestrator, "requirements-python.txt")

# URL de la API de IA - lee de variable de entorno o usa valor por defecto
vURLLlama = os.environ.get("PHIA_AI_API_URL", "http://127.0.0.1:9000/completion")
vProveedorIA = os.environ.get("PHIA_AI_PROVIDER", "llama.cpp")
vAPIKey = os.environ.get("PHIA_AI_API_KEY", None)
vWSMCP = "ws://127.0.0.1:9001"

vRutaBaseWorkspace = "$HOME/PenTests"

# Globals de workspace
vWorkspace = None
vRutaLogs = None
vRutaEvidence = None
vRutaExploits = None
vRutaReport = None

vContadorAcciones = 0


# --------------------------------------------------------
# UTILIDADES DE FICHEROS Y NOMBRES
# --------------------------------------------------------

def fNormalizarNombre(vTexto):
  vTexto = vTexto.strip().lower()
  vTexto = re.sub(r"[^a-z0-9]+", "_", vTexto)
  vTexto = re.sub(r"_+", "_", vTexto)
  vTexto = vTexto.strip("_")
  if not vTexto:
    vTexto = "objetivo"
  return vTexto

def fAsegurarDirectorio(vRuta):
  if not os.path.exists(vRuta):
    os.makedirs(vRuta, exist_ok=True)

def fGuardarJSONEnArchivo(vRutaArchivo, vObjeto):
  with open(vRutaArchivo, "w") as f:
    json.dump(vObjeto, f, indent=2)

def fGuardarTextoEnArchivo(vRutaArchivo, vTexto):
  with open(vRutaArchivo, "w") as f:
    f.write(vTexto)


# --------------------------------------------------------
# VERIFICACIÓN DE DEPENDENCIAS
# --------------------------------------------------------

def fCargarRequirements():
  if not os.path.exists(vRutaRequirements):
    return []

  aPaquetes = []
  with open(vRutaRequirements, "r") as f:
    for vLinea in f:
      vLinea = vLinea.strip()
      if vLinea and not vLinea.startswith("#"):
        aPaquetes.append(vLinea)

  return aPaquetes

def fVerificarPaquete(vPaquete):
  """
  Verifica si un paquete de Python está instalado
  """
  try:
    # Intentar importar el paquete
    __import__(vPaquete)
    return True
  except ImportError:
    return False

def fVerificarDependencias():
  aPaquetes = fCargarRequirements()

  if not aPaquetes:
    return

  print("\n" + "="*60)
  print("VERIFICANDO DEPENDENCIAS DE PENTESTING")
  print("="*60)

  aInstalados = []
  aFaltantes = []

  for vPaq in aPaquetes:
    if fVerificarPaquete(vPaq):
      aInstalados.append(vPaq)
      print(f"✓ {vPaq:<20} [INSTALADO]")
    else:
      aFaltantes.append(vPaq)
      print(f"✗ {vPaq:<20} [NO ENCONTRADO]")

  print("="*60)
  print(f"Instalados: {len(aInstalados)}/{len(aPaquetes)}")

  if aFaltantes:
    print(f"\nPaquetes faltantes ({len(aFaltantes)}):")
    for vPaq in aFaltantes:
      print(f"  - {vPaq}")

    print("\nPara instalar los paquetes faltantes, ejecuta:")
    print(f"  pip3 install {' '.join(aFaltantes)}")

    vRespuesta = input("\n¿Deseas instalar los paquetes faltantes ahora? (s/N): ").strip().lower()

    if vRespuesta in ['s', 'si', 'y', 'yes']:
      print("\nInstalando paquetes de Python...")
      try:
        vCmd = [sys.executable, "-m", "pip", "install"] + aFaltantes
        vResult = subprocess.run(vCmd)

        if vResult.returncode == 0:
          print("\n✓ Paquetes instalados correctamente.")
        else:
          print("\n✗ Hubo un error durante la instalación.")
          print("  Puedes instalarlos manualmente más tarde.")
      except Exception as e:
        print(f"\n✗ Error al instalar: {e}")
        print("  Instálalos manualmente con:")
        print(f"  pip3 install {' '.join(aFaltantes)}")
    else:
      print("\nPuedes instalarlos más tarde con el comando mostrado arriba.")
  else:
    print("\n✓ Todas las dependencias están instaladas.")

  print("="*60 + "\n")


# --------------------------------------------------------
# WORKSPACE
# --------------------------------------------------------

def fCrearWorkspace(vTarget):
  global vWorkspace, vRutaLogs, vRutaEvidence, vRutaExploits, vRutaReport

  vNombre = fNormalizarNombre(vTarget)
  vWorkspace = os.path.join(vRutaBaseWorkspace, vNombre)

  vRutaLogs = os.path.join(vWorkspace, "logs")
  vRutaEvidence = os.path.join(vWorkspace, "evidence")
  vRutaExploits = os.path.join(vWorkspace, "exploits")
  vRutaReport = os.path.join(vWorkspace, "report")

  fAsegurarDirectorio(vWorkspace)
  fAsegurarDirectorio(vRutaLogs)
  fAsegurarDirectorio(vRutaEvidence)
  fAsegurarDirectorio(vRutaExploits)
  fAsegurarDirectorio(vRutaReport)

  print(f"Workspace creado: {vWorkspace}")
  print(f"  Logs:      {vRutaLogs}")
  print(f"  Evidence:  {vRutaEvidence}")
  print(f"  Exploits:  {vRutaExploits}")
  print(f"  Report:    {vRutaReport}")


# --------------------------------------------------------
# CARGA DE TOOLS Y SYSTEM PROMPT
# --------------------------------------------------------

def fCargarTools():
  if not os.path.exists(vRutaTools):
    print(f"No encuentro {vRutaTools}")
    sys.exit(1)

  aTools = []
  vActual = {}

  with open(vRutaTools, "r") as f:
    for vLinea in f:
      vLinea = vLinea.strip()
      if not vLinea:
        if vActual:
          aTools.append(vActual)
          vActual = {}
        continue

      if vLinea.startswith("NAME="):
        vActual["name"] = vLinea.split("=",1)[1]
      elif vLinea.startswith("DESC="):
        vActual["desc"] = vLinea.split("=",1)[1]
      elif vLinea.startswith("ARGS="):
        vActual["args"] = vLinea.split("=",1)[1]

    if vActual:
      aTools.append(vActual)

  return aTools

def fCargarSystemPrompt():
  if not os.path.exists(vRutaSystemPrompt):
    print(f"No encuentro {vRutaSystemPrompt}")
    sys.exit(1)

  with open(vRutaSystemPrompt, "r") as f:
    return f.read()

def fConstruirPromptTotal(vSystemPromptBase, aTools):
  aLineas = []
  aLineas.append(vSystemPromptBase.strip())
  aLineas.append("\n\n=== HERRAMIENTAS MCP DISPONIBLES ===\n")

  for vTool in aTools:
    vNombre = vTool["name"]
    vDesc = vTool.get("desc", "Sin descripción")
    aArgs = vTool.get("args", "").split(",")

    aLineas.append(f"- {vNombre}")
    aLineas.append(f"  Descripción: {vDesc}")
    aLineas.append("  Uso (formato JSON):")
    aLineas.append("  {")
    aLineas.append(f'    "tool": "{vNombre}",')
    aLineas.append('    "arguments": {')
    for vArg in aArgs:
      vArg = vArg.strip()
      if vArg:
        aLineas.append(f'      "{vArg}": "VALOR",')
    aLineas.append("    }")
    aLineas.append("  }\n")

  aLineas.append("Cuando quieras usar una herramienta, devuelve SOLO el JSON exacto del tool-call.")
  aLineas.append("No expliques el comando, no des texto adicional, solo el JSON.")

  return "\n".join(aLineas)


# --------------------------------------------------------
# COMUNICACIÓN CON LLAMA.CPP
# --------------------------------------------------------

def fEnviarALlama(vPromptCompleto, vEntradaUsuario):
  vPromptFinal = vPromptCompleto + "\n\nUsuario:\n" + vEntradaUsuario + "\nAsistente:"

  vPayload = {
    "prompt": vPromptFinal,
    "temperature": 0.0,  # Más determinista
    "top_p": 0.9,
    "top_k": 40,
    "n_predict": 300,  # Reducido para respuestas más cortas
    "stop": ["\nUsuario:", "Usuario:", "\n\n"]  # Detener en estas cadenas
  }

  print(f"\n[DEBUG] Enviando petición a {vURLLlama}")
  print(f"[DEBUG] Longitud del prompt: {len(vPromptFinal)} caracteres")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida")

    # Ajusta esta parte según el formato real de la respuesta de tu llama.cpp.
    if "content" in vJSON:
      return vJSON["content"]
    else:
      # Fallback simple
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con llama.cpp")
    return "ERROR: El modelo tardó demasiado en responder."
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con llama.cpp: {e}")
    return f"ERROR: {str(e)}"


def fEnviarADeepSeek(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a la API de DeepSeek usando formato compatible con OpenAI
  """
  if not vAPIKey:
    return "ERROR: Se requiere API key para usar DeepSeek. Configura PHIA_AI_API_KEY."

  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "deepseek-chat",
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300,
    "stream": False
  }

  vHeaders = {
    "Authorization": f"Bearer {vAPIKey}",
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a DeepSeek API")
  print(f"[DEBUG] Modelo: deepseek-chat")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de DeepSeek")

    # La respuesta de DeepSeek sigue el formato de OpenAI
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con DeepSeek API")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de DeepSeek: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con DeepSeek: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAOpenAI(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a la API de OpenAI usando formato compatible
  """
  if not vAPIKey:
    return "ERROR: Se requiere API key para usar OpenAI. Configura PHIA_AI_API_KEY."

  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "gpt-4o-mini",  # Modelo más económico y rápido
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300
  }

  vHeaders = {
    "Authorization": f"Bearer {vAPIKey}",
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a OpenAI API")
  print(f"[DEBUG] Modelo: gpt-4o-mini")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de OpenAI")

    # La respuesta de OpenAI sigue el formato estándar
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con OpenAI API")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de OpenAI: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con OpenAI: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAOllama(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a Ollama usando formato compatible con OpenAI
  """
  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "llama3.2",  # Modelo por defecto, puede cambiarse
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300,
    "stream": False
  }

  vHeaders = {
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a Ollama")
  print(f"[DEBUG] Modelo: llama3.2")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de Ollama")

    # Ollama usa formato compatible con OpenAI
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    # Formato alternativo de Ollama (sin /v1/)
    elif "message" in vJSON and "content" in vJSON["message"]:
      return vJSON["message"]["content"]
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con Ollama")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.ConnectionError:
    print(f"[ERROR] No se puede conectar con Ollama")
    print(f"[ERROR] Asegúrate de que Ollama esté corriendo: ollama serve")
    return "ERROR: No se puede conectar con Ollama. ¿Está corriendo?"
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de Ollama: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con Ollama: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAVLLM(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a vLLM usando formato compatible con OpenAI
  """
  # Construir mensajes en formato chat
  aMensajes = [
    {
      "role": "system",
      "content": vPromptCompleto
    },
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "model",  # vLLM ignora este campo pero es requerido
    "messages": aMensajes,
    "temperature": 0.0,  # Determinista
    "max_tokens": 300,
    "stream": False
  }

  vHeaders = {
    "Content-Type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a vLLM")
  print(f"[DEBUG] URL: {vURLLlama}")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de vLLM")

    # vLLM usa formato compatible con OpenAI
    if "choices" in vJSON and len(vJSON["choices"]) > 0:
      vContenido = vJSON["choices"][0]["message"]["content"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con vLLM")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.ConnectionError:
    print(f"[ERROR] No se puede conectar con vLLM")
    print(f"[ERROR] Asegúrate de que vLLM esté corriendo en: {vURLLlama}")
    return "ERROR: No se puede conectar con vLLM. ¿Está corriendo?"
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de vLLM: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con vLLM: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAClaude(vPromptCompleto, vEntradaUsuario):
  """
  Envía el prompt a la API de Claude (Anthropic) usando su formato nativo
  """
  if not vAPIKey:
    return "ERROR: Se requiere API key para usar Claude. Configura PHIA_AI_API_KEY."

  # Claude usa un formato diferente: system es un parámetro separado
  aMensajes = [
    {
      "role": "user",
      "content": vEntradaUsuario
    }
  ]

  vPayload = {
    "model": "claude-3-5-sonnet-20241022",  # Modelo más reciente
    "system": vPromptCompleto,  # System prompt es parámetro separado en Claude
    "messages": aMensajes,
    "max_tokens": 300,  # Requerido por Claude
    "temperature": 0.0  # Determinista
  }

  # Claude usa headers diferentes
  vHeaders = {
    "x-api-key": vAPIKey,  # Claude usa x-api-key en lugar de Authorization Bearer
    "anthropic-version": "2023-06-01",  # Versión de la API requerida
    "content-type": "application/json"
  }

  print(f"\n[DEBUG] Enviando petición a Claude API")
  print(f"[DEBUG] Modelo: claude-3-5-sonnet-20241022")

  try:
    vResp = requests.post(vURLLlama, json=vPayload, headers=vHeaders, timeout=120)
    vResp.raise_for_status()
    vJSON = vResp.json()

    print(f"[DEBUG] Respuesta recibida de Claude")

    # Claude devuelve el contenido en content[0].text
    if "content" in vJSON and len(vJSON["content"]) > 0:
      vContenido = vJSON["content"][0]["text"]
      return vContenido
    else:
      print(f"[DEBUG] Formato inesperado, campos disponibles: {vJSON.keys()}")
      return json.dumps(vJSON)

  except requests.exceptions.Timeout:
    print(f"[ERROR] Timeout al comunicarse con Claude API")
    return "ERROR: El modelo tardó demasiado en responder."
  except requests.exceptions.HTTPError as e:
    print(f"[ERROR] Error HTTP de Claude: {e}")
    print(f"[ERROR] Respuesta: {e.response.text}")
    return f"ERROR: {str(e)}"
  except Exception as e:
    print(f"[ERROR] Error al comunicarse con Claude: {e}")
    return f"ERROR: {str(e)}"


def fEnviarAIA(vPromptCompleto, vEntradaUsuario):
  """
  Función dispatcher que envía el prompt al proveedor de IA configurado
  """
  if vProveedorIA == "ollama":
    return fEnviarAOllama(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "vllm":
    return fEnviarAVLLM(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "deepseek":
    return fEnviarADeepSeek(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "openai":
    return fEnviarAOpenAI(vPromptCompleto, vEntradaUsuario)
  elif vProveedorIA == "claude":
    return fEnviarAClaude(vPromptCompleto, vEntradaUsuario)
  else:
    # Por defecto usa llama.cpp
    return fEnviarALlama(vPromptCompleto, vEntradaUsuario)


# --------------------------------------------------------
# COMUNICACIÓN CON MCP
# --------------------------------------------------------

async def fVerificarMCPServer():
  """Verifica si el MCP server está corriendo"""
  try:
    async with websockets.connect(vWSMCP, open_timeout=2) as vWS:
      # Si la conexión es exitosa, el server está corriendo
      return True
  except Exception:
    return False

async def fLlamarMCP(vTool, vArgs):
  try:
    async with websockets.connect(vWSMCP) as vWS:
      vMsg = {
        "jsonrpc": "2.0",
        "id": 1,
        "method": "tools/call",
        "params": {
          "tool": vTool,
          "arguments": vArgs
        }
      }
      await vWS.send(json.dumps(vMsg))
      vRespuesta = await vWS.recv()
      return json.loads(vRespuesta)
  except ConnectionRefusedError:
    print("\n" + "="*70)
    print("ERROR: MCP SERVER NO ESTÁ CORRIENDO")
    print("="*70)
    print("El orquestador no puede conectarse al MCP server en:")
    print(f"  {vWSMCP}")
    print("\nPara iniciar el MCP server, abre otra terminal y ejecuta:")
    print(f"  cd {os.path.join(os.path.dirname(os.path.abspath(__file__)))}")
    print("  ./mcp-server-start.sh")
    print("\nLuego vuelve a ejecutar el framework.")
    print("="*70 + "\n")
    sys.exit(1)
  except Exception as e:
    print(f"\n[ERROR] Error al comunicarse con MCP server: {e}\n")
    return {"error": str(e)}


# --------------------------------------------------------
# DETECTAR TOOL-CALL DEL MODELO
# --------------------------------------------------------

def fDetectarToolCall(vTexto):
  """
  Detecta y extrae un tool-call JSON del texto del modelo.
  El JSON puede venir mezclado con texto adicional.
  """
  # Primero intenta parsear todo el texto como JSON
  try:
    vJSON = json.loads(vTexto.strip())
    if "tool" in vJSON and "arguments" in vJSON:
      return vJSON
  except Exception:
    pass

  # Buscar todas las posiciones donde aparece '{'
  vPosiciones = [i for i, c in enumerate(vTexto) if c == '{']

  # Para cada '{', intentar extraer un JSON válido
  for vInicio in vPosiciones:
    # Intentar encontrar el '}' correspondiente
    vContador = 0
    for vFin in range(vInicio, len(vTexto)):
      if vTexto[vFin] == '{':
        vContador += 1
      elif vTexto[vFin] == '}':
        vContador -= 1
        if vContador == 0:
          # Encontramos un par completo de llaves
          vJSONStr = vTexto[vInicio:vFin+1]
          try:
            vJSON = json.loads(vJSONStr)
            if isinstance(vJSON, dict) and "tool" in vJSON and "arguments" in vJSON:
              print(f"[DEBUG] Tool-call detectado: {vJSON['tool']}")
              print(f"[DEBUG] JSON extraído: {vJSONStr}")
              return vJSON
          except Exception:
            # Este fragmento no es JSON válido, continuar
            pass
          break

  print(f"[DEBUG] No se detectó tool-call válido en la respuesta")
  print(f"[DEBUG] Texto recibido: {vTexto[:200]}...")
  return None


# --------------------------------------------------------
# PROCESAR RESULTADO DE MCP (LOGS, EVIDENCIAS, INFORME)
# --------------------------------------------------------

def fGenerarNombreAccion(vTool):
  global vContadorAcciones
  vContadorAcciones += 1
  vTimestamp = int(time.time())
  vNombre = f"{vContadorAcciones:04d}_{fNormalizarNombre(vTool)}_{vTimestamp}"
  return vNombre

def fProcesarResultadoMCP(vTool, vArgs, vResultado):
  if vWorkspace is None:
    return

  vNombreAccion = fGenerarNombreAccion(vTool)

  # 1) Log general de tool-call (args + resultado bruto)
  vRutaLogJSON = os.path.join(vRutaLogs, f"{vNombreAccion}_toolcall.json")
  vObjetoLog = {
    "tool": vTool,
    "arguments": vArgs,
    "result": vResultado
  }
  fGuardarJSONEnArchivo(vRutaLogJSON, vObjetoLog)

  # 2) Si hay request/response HTTP, los guardamos como evidencias
  try:
    vResult = vResultado.get("result", {})
    vOutput = vResult.get("output", {})

    vRequest = vOutput.get("request")
    vResponse = vOutput.get("response")

    if vRequest:
      vRutaReq = os.path.join(vRutaEvidence, f"{vNombreAccion}_request.txt")
      fGuardarTextoEnArchivo(vRutaReq, vRequest)

    if vResponse:
      vRutaRes = os.path.join(vRutaEvidence, f"{vNombreAccion}_response.txt")
      if isinstance(vResponse, str):
        fGuardarTextoEnArchivo(vRutaRes, vResponse)
      else:
        fGuardarTextoEnArchivo(vRutaRes, json.dumps(vResponse, indent=2))

  except Exception:
    pass

  # 3) Opcional: si el MCP devuelve algo como "evidence" o "note", también lo guardamos
  try:
    vResult = vResultado.get("result", {})
    vEvidence = vResult.get("evidence")
    if vEvidence:
      vRutaEv = os.path.join(vRutaEvidence, f"{vNombreAccion}_extra.txt")
      if isinstance(vEvidence, str):
        fGuardarTextoEnArchivo(vRutaEv, vEvidence)
      else:
        fGuardarTextoEnArchivo(vRutaEv, json.dumps(vEvidence, indent=2))
  except Exception:
    pass


# --------------------------------------------------------
# MODO AUTOMÁTICO
# --------------------------------------------------------

async def fModoAutomatico(vTarget, vSystemPromptConObjetivo):
  """
  Ejecuta automáticamente una secuencia completa de pentesting
  """
  print("\n" + "="*70)
  print("MODO AUTOMÁTICO ACTIVADO")
  print("="*70)
  print("El framework ejecutará automáticamente:")
  print("  1. Reconocimiento inicial (HTTP GET)")
  print("  2. Escaneo de directorios")
  print("  3. Búsqueda de vulnerabilidades SQL Injection")
  print("  4. Búsqueda de vulnerabilidades XSS")
  print("  5. Generación de informe final")
  print("="*70 + "\n")

  aSecuenciaAcciones = [
    "Haz un HTTP GET al objetivo para obtener la página principal y analizar las cabeceras",
    "Escanea directorios y archivos comunes usando dirscan",
    "Analiza todos los parámetros encontrados en busca de SQL Injection",
    "Analiza todos los parámetros encontrados en busca de XSS",
    "Resume todos los hallazgos y genera el informe final en Markdown y HTML"
  ]

  for i, vAccion in enumerate(aSecuenciaAcciones, 1):
    print(f"\n{'='*70}")
    print(f"PASO {i}/{len(aSecuenciaAcciones)}: {vAccion}")
    print(f"{'='*70}\n")

    vSalida = fEnviarAIA(vSystemPromptConObjetivo, vAccion)
    vToolCall = fDetectarToolCall(vSalida)

    if vToolCall:
      vTool = vToolCall["tool"]
      vArgs = vToolCall["arguments"]

      # Añadir workspace si la tool lo necesita
      if "workspace" not in vArgs and vTool in ["dirscan", "report.write", "report.final_md", "report.final_html", "exploit.write_poc", "exploit.run"]:
        vArgs["workspace"] = vWorkspace

      print(f"[Orquestador] Ejecutando: {vTool}")
      print(f"[Orquestador] Argumentos: {vArgs}\n")

      vResultado = await fLlamarMCP(vTool, vArgs)
      fProcesarResultadoMCP(vTool, vArgs, vResultado)

      print("[Resultado MCP]:")
      print(json.dumps(vResultado, indent=2))

      # Analizar resultado
      vNuevoContexto = (
        f"Resultado de {vTool}:\n"
        f"{json.dumps(vResultado, indent=2)}\n"
        "Analiza este resultado brevemente."
      )
      vAnalisis = fEnviarAIA(vSystemPromptConObjetivo, vNuevoContexto)
      print(f"\n[Análisis]:\n{vAnalisis}\n")

    else:
      print(f"\n[Modelo]:\n{vSalida}\n")

    # Pausa entre acciones
    import time
    time.sleep(2)

  print("\n" + "="*70)
  print("PENTESTING AUTOMÁTICO COMPLETADO")
  print("="*70)
  print(f"\nResultados guardados en: {vWorkspace}")
  print(f"  - Logs:      {vRutaLogs}")
  print(f"  - Evidencia: {vRutaEvidence}")
  print(f"  - Reportes:  {vRutaReport}")
  print("="*70 + "\n")


# --------------------------------------------------------
# LOOP PRINCIPAL
# --------------------------------------------------------

async def fLoop():
  aTools = fCargarTools()
  vSystemPromptBase = fCargarSystemPrompt()
  vSystemPromptFinal = fConstruirPromptTotal(vSystemPromptBase, aTools)

  print("Orchestrator PT-Web avanzado inicializado.")
  print("Tools cargadas desde tools.txt:")
  for vT in aTools:
    print(f" - {vT['name']}")

  # Verificar dependencias del sistema
  fVerificarDependencias()

  # Verificar que el MCP server esté corriendo
  print("Verificando conexión con MCP server...")
  vMCPDisponible = await fVerificarMCPServer()

  if not vMCPDisponible:
    print("\n" + "="*70)
    print("ERROR: MCP SERVER NO ESTÁ CORRIENDO")
    print("="*70)
    print("El orquestador necesita que el MCP server esté corriendo para")
    print("ejecutar las herramientas de pentesting.\n")
    print(f"MCP Server esperado en: {vWSMCP}\n")
    print("Para iniciar el MCP server, abre otra terminal y ejecuta:")
    vRutaMCP = os.path.join(os.path.dirname(os.path.abspath(__file__)), "mcp-server.py")
    print(f"  cd {os.path.dirname(vRutaMCP)}")
    print("  python3 mcp-server.py")
    print("\nLuego vuelve a ejecutar el framework.")
    print("="*70 + "\n")
    sys.exit(1)
  else:
    print("✓ MCP server detectado y funcionando\n")

  vTarget = input("Objetivo (URL o dominio, por ejemplo https://ejemplo.com): ").strip()
  if not vTarget:
    print("Objetivo vacío. Saliendo.")
    return

  fCrearWorkspace(vTarget)

  # Agregar el objetivo al system prompt
  vSystemPromptConObjetivo = (
    vSystemPromptFinal +
    f"\n\n=== OBJETIVO ACTUAL ===\n"
    f"URL/Dominio: {vTarget}\n"
    f"Workspace: {vWorkspace}\n"
    f"\nEste es el objetivo que debes analizar. "
    f"Cuando el usuario te pida realizar acciones, úsalas contra este objetivo: {vTarget}\n"
    f"No pidas la URL nuevamente, ya la tienes: {vTarget}\n"
  )

  # Preguntar modo de operación
  print("\n" + "="*70)
  print("MODO DE OPERACIÓN")
  print("="*70)
  print("  [1] Modo Automático - El framework ejecuta todo el pentesting")
  print("  [2] Modo Manual     - Tú escribes los comandos")
  print("="*70)

  vModo = input("\nSelecciona modo [1/2] (por defecto: 1): ").strip()

  if vModo == "2":
    # Modo Manual
    print("\nModo Manual activado.")
    print("Escribe tus instrucciones de pentesting web.")
    print("Ejemplo: 'Haz reconocimiento inicial del objetivo.'\n")

    while True:
      vEntrada = input("\nNiPeGun-PTWeb> ")

      if not vEntrada:
        continue

      vSalida = fEnviarAIA(vSystemPromptConObjetivo, vEntrada)
      vToolCall = fDetectarToolCall(vSalida)

      if vToolCall:
        vTool = vToolCall["tool"]
        vArgs = vToolCall["arguments"]

        # Añadir workspace si la tool lo necesita
        if "workspace" not in vArgs and vTool in ["dirscan", "report.write", "report.final_md", "report.final_html", "exploit.write_poc", "exploit.run"]:
          vArgs["workspace"] = vWorkspace

        print(f"[Orquestador] El modelo pide tool: {vTool}")
        print(f"[Orquestador] Args: {vArgs}\n")

        vResultado = await fLlamarMCP(vTool, vArgs)

        fProcesarResultadoMCP(vTool, vArgs, vResultado)

        print("[Resultado MCP]:")
        print(json.dumps(vResultado, indent=2))

        vNuevoContexto = (
          f"Resultado de {vTool} contra el objetivo {vTarget}:\n"
          f"{json.dumps(vResultado, indent=2)}\n"
          "Analiza este resultado, identifica posibles vulnerabilidades, "
          "clasifícalas (OWASP, CWE, severidad) y decide el siguiente paso."
        )
        vContinuacion = fEnviarAIA(vSystemPromptConObjetivo, vNuevoContexto)

        print("\n[Modelo]:")
        print(vContinuacion)

      else:
        print("\n[Modelo]:")
        print(vSalida)

  else:
    # Modo Automático (por defecto)
    await fModoAutomatico(vTarget, vSystemPromptConObjetivo)


if __name__ == "__main__":
  try:
    asyncio.run(fLoop())
  except KeyboardInterrupt:
    sys.exit(0)
